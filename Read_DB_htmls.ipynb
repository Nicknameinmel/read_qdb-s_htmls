{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae04641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json \n",
    "import os\n",
    "#read\n",
    "path = r'C:\\Users\\kyle.liu\\SearchEngine\\page1'      # 输入文件夹地址\n",
    "files = os.listdir(path)   # 读入文件夹\n",
    "num_png = len(files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3ec6ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_r(class_list):\n",
    "#    try:\n",
    "        if len(class_list) <1:\n",
    "            pass\n",
    "#1+1\n",
    "        elif len(class_list) == 6:\n",
    "            r1 = soup.find(\"span\", class_=class_list[2])\n",
    "            r2 = soup.find(\"span\", class_=class_list[3])\n",
    "            r3 = soup.find('line')\n",
    "            r4 = soup.find(\"span\", class_=class_list[5]) \n",
    "            r11 = r1.text\n",
    "            r22 = r2.text\n",
    "            if 'right' in str(r3):\n",
    "                r3 = '->'\n",
    "            elif 'left' in str(r3):\n",
    "                r3 = '<-'\n",
    "            r44 = r4.text\n",
    "    #    with open('searchresults.csv', 'a', encoding='gb18030') as c: \n",
    "    #         c.write(json.dumps(r11, ensure_ascii=False) + '\\n') \n",
    "    #         c.write(json.dumps(r22, ensure_ascii=True) + '\\n') \n",
    "    #         c.write(json.dumps(r33, ensure_ascii=False) + '\\n') \n",
    "            print(r1.text+'('+r2.text+' )'+r3+r4.text)\n",
    "#1+1+1\n",
    "        elif len(class_list) == 9 :\n",
    "            rx = soup.find(class_=class_list[0])\n",
    "            r1 = soup.find_all('line')\n",
    "            r2 = soup.find_all(\"span\", class_=class_list[3])\n",
    "            r3 = soup.find(\"span\", class_=class_list[5])\n",
    "            r4 = soup.find(\"span\", class_=class_list[6])\n",
    "            r5 = soup.find(\"span\", class_=class_list[8])\n",
    "            for i in range(0,2):\n",
    "                if 'right' in str(r1[i]):\n",
    "                    r1[i]= '->'\n",
    "                elif 'left' in str(r1[i]):\n",
    "                    r1[i]= '<-'\n",
    "# 切割                    \n",
    "#            print(rx.text.split(\" \")[0])\n",
    "                    \n",
    "                r22 = r2[i].text\n",
    "            r33 = r3.text\n",
    "            r55 = r5.text\n",
    "            #with open('searchresults.csv', 'a', encoding='gb18030') as c: \n",
    "            #         c.write(json.dumps(r11, ensure_ascii=False) + '\\n') \n",
    "            #         c.write(json.dumps(r22, ensure_ascii=True) + '\\n') \n",
    "            #         c.write(json.dumps(r33, ensure_ascii=False) + '\\n') \n",
    "#            print(rx.text.split(\" \")[0]+r1[0]+'('+r2.text+' )'+r3.text)\n",
    "#            print(r3.text+'('+r4.text+' )'+r1[1]+r5.text)\n",
    "            print(rx.text.split(\" \")[0]+r1[0]+'('+r2[0].text+' )'+r3.text+'('+r2[1].text+' )'+r1[1]+r5.text)\n",
    "#    except AttributeError:\n",
    "#        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a91d9e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "任东馗( 法定代表人;经理;董事长 )->鄂尔多斯市乾恒兴实业有限公司\n",
      "鄂尔多斯市乾东能源开发有限公司( 工商股东 )->鄂尔多斯市乾东能源开发有限公司\n",
      "西宁华旺商贸有限责任公司( 工商股东 )->西宁华旺商贸有限责任公司\n",
      "刘波( 监事 )->鄂尔多斯市乾恒兴实业有限公司\n",
      "杨秀宁( 监事会主席 )->鄂尔多斯市乾恒兴实业有限公司\n",
      "朱玉芳( 监事 )->鄂尔多斯市乾恒兴实业有限公司\n",
      "孙琳( 董事 )->鄂尔多斯市乾恒兴实业有限公司\n",
      "谭云( 董事 )->鄂尔多斯市乾恒兴实业有限公司\n",
      "姚靖宇( 董事 )->鄂尔多斯市乾恒兴实业有限公司\n",
      "鄂尔多斯市纳林河矿业开发有限责任公司<-( 经理;执行董事 )任东馗( 法定代表人;经理;董事长 )->鄂尔多斯市乾恒兴实业有限公司\n"
     ]
    }
   ],
   "source": [
    "for p in range(len (files)):\n",
    "    soup=BeautifulSoup(open(''+path+'\\part['+str(p+1)+'].csv',encoding='gb18030'),features=\"lxml\") \n",
    "\n",
    "    # get all tags\n",
    "    tags = {tag.name for tag in soup.find_all()}\n",
    "    #\n",
    "    class_list =[ [] for x in range(len (files))]\n",
    "\n",
    "    # iterate all tags\n",
    "    for tag in tags:\n",
    "\n",
    "        # find all element of tag\n",
    "        for c in soup.find_all( tag ):\n",
    "\n",
    "            # if tag has attribute of class\n",
    "            if c.has_attr( \"class\" ):\n",
    "                if len( c['class'] ) != 0:               \n",
    "#                    print(p)\n",
    "#                    print(c['class'])\n",
    "                    class_list[p].append(\" \".join( c['class']))\n",
    "        print_r(class_list[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json \n",
    "soup=BeautifulSoup(open('eeds.html',encoding='utf-8'),features=\"lxml\") \n",
    "#\"html5lib\"\n",
    "\n",
    "for child in soup.find_all('div', class_=\"route path-desc\"):\n",
    "            print(repr(child))\n",
    "            with open('part.csv', 'a', encoding='gb18030') as c:\n",
    "                c.write(json.dumps(repr(child), ensure_ascii=False) + '\\n')\n",
    "\n",
    "for i in range(10):\n",
    "    html = df.iat[i,0]\n",
    "    i_str = str(i+1)\n",
    "    with open('part['+i_str+'].csv', 'a', encoding='gb18030') as c:\n",
    "                c.write(json.dumps(html, ensure_ascii=False) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
